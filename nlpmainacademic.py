# -*- coding: utf-8 -*-
"""NLPmainacademic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10JYB6QJBc772huLjFJJv025mXlzw2RXJ
"""

# Install a version of datasets that still supports loading scripts
!pip install datasets==2.19.0 pandas

# IMPORTANT: After running this, go to "Runtime" > "Restart Session" (or "Restart Runtime")
# in the menu so the new version takes effect.

import re
import pandas as pd
from datasets import load_dataset

class ReviewerRecommenderDataLoader:
    def __init__(self, dataset_name="allenai/peer_read"):
        print(f"Loading dataset: {dataset_name}...")
        # 'trust_remote_code=True' is required for this dataset script
        self.dataset = load_dataset(dataset_name, "reviews", trust_remote_code=True)
        print("Dataset loaded successfully.")

    def preprocess_text(self, text):
        if not text:
            return ""
        text = text.lower()
        text = re.sub(r'[^a-z\s]', '', text)
        text = " ".join(text.split())
        return text

    def get_processed_dataframe(self, split='train'):
        data = []
        for entry in self.dataset[split]:
            paper_title = entry.get('title', '')
            abstract = entry.get('abstract', '')

            # --- THE FIX IS HERE ---
            # 1. Extract Authors (These are your potential "Reviewers" for other papers)
            authors = entry.get('authors', [])

            # 2. Handle Reviews (Check if they are strings or objects)
            raw_reviews = entry.get('reviews', [])
            review_texts = []

            for r in raw_reviews:
                if isinstance(r, str):
                    # If it's just a string, it's the review text
                    review_texts.append(r)
                elif isinstance(r, dict):
                    # If it's a dict, get the text from the 'comments' key
                    review_texts.append(r.get('comments', ''))

            # Combine Title + Abstract for the embedding model
            full_text = f"{paper_title} {abstract}"

            data.append({
                'paper_id': entry.get('id'),
                'title': paper_title,
                'clean_text': self.preprocess_text(full_text),
                'authors': authors,  # New: We track authors to build our "Expert Pool"
                'review_texts': review_texts,
                'acceptance_status': entry.get('accepted')
            })

        return pd.DataFrame(data)

# --- EXECUTION ---
loader = ReviewerRecommenderDataLoader()
df_train = loader.get_processed_dataframe(split='train')

print(f"\nTotal Papers Loaded: {len(df_train)}")
print("\nSample Data (First Row):")
print(df_train[['title', 'authors']].iloc[0])

!pip install sentence-transformers

from sentence_transformers import SentenceTransformer
import numpy as np

# 1. Load the "Specter" model (The SciBERT specialist for papers)
# This downloads about 400MB of model weights
print("Loading model: allenai/specter...")
model = SentenceTransformer('allenai/specter')
print("Model loaded successfully.")

# 2. Prepare the data
# We combine Title + Abstract because the model needs both to understand context
# Let's verify we are using the 'clean_text' column you created earlier
papers_to_encode = df_train['clean_text'].tolist()

# --- TEST RUN: Process only the first 100 papers ---
subset_size = 100
print(f"\nGeneratng embeddings for the first {subset_size} papers...")

# The encode function does the magic.
# convert_to_tensor=False gives us standard numpy arrays (easier to work with for now)
embeddings = model.encode(papers_to_encode[:subset_size], show_progress_bar=True)

# 3. Add Vectors back to the DataFrame
# We create a new column to store these number lists
df_subset = df_train.iloc[:subset_size].copy()
df_subset['embedding'] = list(embeddings)

# --- INSPECTION ---
print("\nSuccess! Vectors generated.")
print(f"Embedding Shape for one paper: {embeddings[0].shape}")
print("First 5 numbers of the first paper's vector:")
print(embeddings[0][:5])

# View the dataframe with the new data
print(df_subset[['title', 'embedding']].head(2))

import numpy as np
import pandas as pd
import random

# We work with 'df_subset' from the previous step
print(f"Building profiles from {len(df_subset)} papers...")

# --- STEP 1: HANDLE MISSING AUTHORS (The Fix) ---
# Since the dataset is anonymized, we create "Synthetic Authors"
# to act as our pool of experts for this tutorial.
def get_or_simulate_authors(row_authors, row_index):
    # If authors exist, use them
    if len(row_authors) > 0:
        return row_authors
    # If empty, assign a "Simulated Researcher" based on the row index
    # We assign the same researcher to multiple papers to simulate "Experts" having a history
    # This math (row_index % 20) creates 20 "repeat experts" across the 100 papers
    simulated_id = f"Researcher_{row_index % 20}"
    return [simulated_id]

# Apply the fix
df_subset['authors'] = [
    get_or_simulate_authors(r, i)
    for i, r in enumerate(df_subset['authors'])
]

# --- STEP 2: BUILD PROFILES ---
author_vectors = {}

for index, row in df_subset.iterrows():
    paper_vector = row['embedding']
    current_authors = row['authors']

    for author in current_authors:
        if author not in author_vectors:
            author_vectors[author] = []
        author_vectors[author].append(paper_vector)

# --- STEP 3: AGGREGATE EXPERTISE ---
reviewer_profiles = []

print(f"Processing {len(author_vectors)} unique experts...")

for author, vectors in author_vectors.items():
    # Average the vectors to create the "Expertise Profile"
    avg_vector = np.mean(vectors, axis=0)

    reviewer_profiles.append({
        'reviewer_id': author,
        'paper_count': len(vectors),
        'profile_embedding': avg_vector
    })

df_reviewers = pd.DataFrame(reviewer_profiles)

# --- INSPECTION ---
print("\nReviewer Profiles Built Successfully!")
print(df_reviewers.head(3))
print(f"\nExample: '{df_reviewers.iloc[0]['reviewer_id']}' now has an expertise vector.")

from sklearn.metrics.pairwise import cosine_similarity

def get_recommendations(new_paper_vector, reviewers_df, top_k=5):
    """
    Finds the top_k reviewers whose expertise aligns with the new paper.
    """
    # 1. Calculate Similarity
    # We reshape the paper vector to (1, 768) because the function expects a 2D array
    paper_vec_reshaped = new_paper_vector.reshape(1, -1)

    # This computes the similarity between the ONE paper and ALL reviewers at once
    # Result is a list of scores like [[0.85, 0.12, 0.45, ...]]
    scores = cosine_similarity(paper_vec_reshaped, list(reviewers_df['profile_embedding']))[0]

    # 2. Rank the Reviewers
    # We create a temporary list of (Reviewer Name, Score)
    ranked_reviewers = list(zip(reviewers_df['reviewer_id'], scores))

    # Sort by score (highest first)
    ranked_reviewers = sorted(ranked_reviewers, key=lambda x: x[1], reverse=True)

    return ranked_reviewers[:top_k]

# --- TEST THE SYSTEM ---

# Let's pick a random paper from our dataset to act as the "New Submission"
test_paper_index = 5  # You can change this number
target_paper = df_subset.iloc[test_paper_index]
target_embedding = target_paper['embedding']

print(f"Finding reviewers for Paper: '{target_paper['title']}'")
print(f"True Author (Synthetic): {target_paper['authors']}\n")

# Run the recommendation engine
recommendations = get_recommendations(target_embedding, df_reviewers)

print("--- TOP RECOMMENDED REVIEWERS ---")
for rank, (reviewer, score) in enumerate(recommendations, 1):
    print(f"#{rank}: {reviewer} (Confidence Score: {score:.4f})")

class ReviewerManager:
    def __init__(self, reviewers_df, max_papers_per_reviewer=3):
        self.reviewers_df = reviewers_df
        self.max_load = max_papers_per_reviewer
        # Track how many papers each reviewer has been assigned
        self.current_loads = {r_id: 0 for r_id in reviewers_df['reviewer_id']}

    def is_conflicted(self, reviewer_id, paper_authors):
        """
        Check for Conflict of Interest (CoI).
        Simple Rule: A reviewer cannot review their own paper.
        """
        if reviewer_id in paper_authors:
            return True
        return False

    def get_balanced_recommendations(self, new_paper_vector, paper_authors, top_k=3):
        """
        Returns recommendations that respect Conflicts and Workload limits.
        """
        # 1. Get ALL raw scores first (using the logic we built before)
        # Reshape for scikit-learn
        paper_vec_reshaped = new_paper_vector.reshape(1, -1)
        scores = cosine_similarity(paper_vec_reshaped, list(self.reviewers_df['profile_embedding']))[0]

        # Zip scores with IDs
        candidates = []
        for idx, score in enumerate(scores):
            r_id = self.reviewers_df.iloc[idx]['reviewer_id']
            candidates.append((r_id, score))

        # Sort by score (Highest first)
        candidates = sorted(candidates, key=lambda x: x[1], reverse=True)

        # 2. Filter & Assign
        final_picks = []

        for r_id, score in candidates:
            # STOP if we have enough reviewers
            if len(final_picks) >= top_k:
                break

            # CHECK 1: Conflict of Interest
            if self.is_conflicted(r_id, paper_authors):
                continue  # Skip this person

            # CHECK 2: Workload Limit
            if self.current_loads.get(r_id, 0) >= self.max_load:
                continue  # Skip (Too busy)

            # If they pass checks, add them
            final_picks.append((r_id, score))

        return final_picks

    def assign_reviewer(self, reviewer_id):
        """
        Official assignment - increments their load.
        """
        self.current_loads[reviewer_id] += 1

# --- TEST THE UPGRADED SYSTEM ---

# Initialize the Manager
manager = ReviewerManager(df_reviewers, max_papers_per_reviewer=2)

# Let's try to assign reviewers to the same paper as before
print(f"Assigning Reviewers for: '{target_paper['title']}'")
print(f"Paper Authors (Conflicted): {target_paper['authors']}\n")

safe_recommendations = manager.get_balanced_recommendations(
    target_embedding,
    target_paper['authors']
)

print("--- SAFE & BALANCED RECOMMENDATIONS ---")
for rank, (reviewer, score) in enumerate(safe_recommendations, 1):
    print(f"#{rank}: {reviewer} (Score: {score:.4f})")
    # Simulate assigning them so we can test load balancing later
    manager.assign_reviewer(reviewer)

print(f"\nUpdated Workloads: {manager.current_loads}")